#!/bin/bash

# job identification number
id=1
# check if id is given as input parameter
# (like when running in condor job system)
if [ $# -ne 0 ]
then
    id=$1
fi

tar -xzf hic_HQ-osg.tar.gz
sleep 2
cd hic_HQ-osg/

# load necessary modules
source /cvmfs/oasis.opensciencegrid.org/osg/modules/lmod/current/init/bash
module purge
module load python/3.5.2 all-pkgs gcc/6.2.0 boost/1.62.0-cxx11 gsl/2.3
module load hdf5/1.8.20-cxx11
module list


export XDG_DATA_HOME="$(pwd)/share"
export PYTHONPATH="$(echo $(pwd)/lib/python*/site-packages)"

cp ../args.conf .
ldd bin/fragPLUSrecomb

./run-events_cD.py args.conf $1

cd ../    
tar czf results$1.tar.gz hic_HQ-osg/results    

transferFile=results$1.tar.gz

## transfer results
## try to be fault-tolerant
#globus() {
#  globus-url-copy \
#  -verbose -create-dest -restart -stall-timeout 30 $transferFile \
#  gsiftp://ntheoryfs01.phy.duke.edu/var/phy/project/nukeserv/yx59/summer2017/run79_AuAu200_param18/condor_00/
#}

#for i in {1..5}; do
#  globus && break
#  sleep 5 && false
#done || \
#  globus -no-data-channel-authentication -no-third-party-transfers
